{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlazeFace_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzF1YgReRXfY2cQBSFYjPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subham913/BlazeFace_/blob/master/BlazeFace_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoJ06UMzCbQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YX-vzctN24k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKuUaRJNUuKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SingleBlazeBlock(x, filter_size, kernel_size = 5, strides = 1):\n",
        "  _x = tf.keras.layers.SeparableConv2D(filters = filter_size, kernel_size=kernel_size, strides = strides, padding = 'same', use_bias=False)(x)\n",
        "  _x = tf.keras.layers.BatchNormalization()(_x)\n",
        "  channel_pad = _x.shape[-1] - x.shape[-1] #######Channel padding for residual connections\n",
        "  if(strides==2):\n",
        "    x = tf.keras.layers.MaxPooling2D()(x)\n",
        "    if(channel_pad):\n",
        "      # x = tf.keras.backend.concatenate([x,tf.zeros((x.shape[0],x.shape[1],x.shape[2],channel_pad),dtype = x.dtype)],axis=-1)\n",
        "      x = tf.keras.backend.concatenate([x,tf.zeros_like(x)],axis=-1) \n",
        "  \n",
        "  x_out = tf.keras.layers.Add()([_x,x])\n",
        "  x_out = tf.keras.layers.Activation(\"relu\")(x_out)\n",
        "  return x_out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W981b3JigMGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DoubleBlazeBlock(x, filter1_size, filter2_size, kernel_size = 5, strides = 1):\n",
        "  \n",
        "  _x1 = tf.keras.layers.SeparableConv2D(filters = filter1_size, kernel_size=kernel_size, strides= strides, padding = 'same', use_bias=False)(x)\n",
        "  _x1 = tf.keras.layers.BatchNormalization()(_x1)\n",
        "  _x1 = tf.keras.layers.Activation(\"relu\")(_x1)\n",
        "\n",
        "  _x2 = tf.keras.layers.SeparableConv2D(filters = filter2_size, kernel_size=kernel_size, strides = 1, padding = 'same', use_bias=False)(_x1)\n",
        "  _x2 = tf.keras.layers.BatchNormalization()(_x2)\n",
        "\n",
        "  channel_pad = _x2.shape[-1] - x.shape[-1]\n",
        "  if(strides==2):\n",
        "    x = tf.keras.layers.MaxPooling2D()(x)\n",
        "    if(channel_pad):\n",
        "      # x = tf.keras.backend.concatenate([x,tf.zeros((x.shape[0],x.shape[1],x.shape[2],channel_pad),dtype = x.dtype)],axis=-1)\n",
        "      x = tf.keras.backend.concatenate([x,tf.zeros_like(x)],axis=-1)\n",
        "  x_out = tf.keras.layers.Add()([_x2,x])\n",
        "  x_out = tf.keras.layers.Activation(\"relu\")(x_out)\n",
        "  return x_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMROBVweSFRo",
        "colab_type": "text"
      },
      "source": [
        "#Test SingleBlazeBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHvC9fJERTFF",
        "colab_type": "code",
        "outputId": "0f1837cc-a977-4519-ad51-0afc0c4ea6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# m  = tf.keras.layers.SeparableConv2D(filters = 24, kernel_size=5, strides = 2, padding = 'same')\n",
        "a = tf.convert_to_tensor(np.random.randn(3,64,64,24),dtype = tf.float32)\n",
        "out = SingleBlazeBlock(a, 48, strides=2)\n",
        "print(out.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 32, 32, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4s97K4ASNDq",
        "colab_type": "text"
      },
      "source": [
        "# Test DoubleBlazeBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0GIq8lkSU7q",
        "colab_type": "code",
        "outputId": "0b33dd85-8a72-4dc3-e96d-f8e98d5a08c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# m  = tf.keras.layers.SeparableConv2D(filters = 24, kernel_size=5, strides = 2, padding = 'same')\n",
        "a = tf.convert_to_tensor(np.random.randn(3,32,32,48),dtype = tf.float32)\n",
        "out = DoubleBlazeBlock(a, 24, 48, strides=1)\n",
        "print(out.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 32, 32, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lbj21D6GTUS",
        "colab_type": "text"
      },
      "source": [
        "#Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AypFHCWRdJvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def network():\n",
        "  inputs = tf.keras.layers.Input(shape=(128,128,3))\n",
        "  x = tf.keras.layers.Conv2D(kernel_size = 5, filters = 24, strides = 2, padding = 'same')(inputs) ## bx64x64x24\n",
        "\n",
        "  x = SingleBlazeBlock(x, 24) ## bx64x64x24\n",
        "  \n",
        "  x = SingleBlazeBlock(x, 24) ## bx64x64x24\n",
        "  \n",
        "  x = SingleBlazeBlock(x, 48, strides = 2) ## bx32x32x48\n",
        "  \n",
        "  x = SingleBlazeBlock(x, 48) ## bx32x32x48\n",
        "  \n",
        "  x = SingleBlazeBlock(x, 48) ## bx32x32x48\n",
        "  \n",
        "\n",
        "  x16 = DoubleBlazeBlock(x, 24, 96, strides = 2) ## bx16x16x96\n",
        "  \n",
        "  _x = DoubleBlazeBlock(x16, 24, 96) ## bx16x16x96\n",
        " \n",
        "  _x = DoubleBlazeBlock(_x, 24, 96) ## bx16x16x96\n",
        "  \n",
        "  _x = DoubleBlazeBlock(_x, 24, 96, strides = 2) ## bx8x8x96\n",
        "  \n",
        "  _x = DoubleBlazeBlock(_x, 24, 96) ## bx8x8x96\n",
        "  \n",
        "  x8 = DoubleBlazeBlock(_x, 24, 96) ## bx8x8x96\n",
        " \n",
        "\n",
        "  ####confidence\n",
        "  x16_conf = tf.keras.layers.Conv2D(kernel_size = 3, filters = 2, strides = 1, padding = 'same', activation = 'sigmoid')(x16) ## bx16x16x2\n",
        " \n",
        "  x8_conf = tf.keras.layers.Conv2D(kernel_size = 3, filters = 6, strides = 1, padding = 'same', activation = 'sigmoid')(x8) ## bx8x8x6\n",
        " \n",
        "\n",
        "  x_conf = tf.keras.layers.concatenate([tf.keras.layers.Reshape((512, 1))(x16_conf),tf.keras.layers.Reshape((384, 1))(x8_conf)],axis=1) ## bx896x1\n",
        "\n",
        "  ###bounding boxes\n",
        "  x16_bboxes = tf.keras.layers.Conv2D(kernel_size = 3, filters = 8, strides = 1, padding = 'same')(x16) ## bx16x16x8\n",
        "  \n",
        "  x8_bboxes = tf.keras.layers.Conv2D(kernel_size = 3, filters = 24, strides = 1, padding = 'same')(x8) ## bx8x8x24\n",
        "  \n",
        "  x_bboxes = tf.keras.layers.concatenate([tf.keras.layers.Reshape((512, 4))(x16_bboxes),tf.keras.layers.Reshape((384, 4))(x8_bboxes)],axis=1) ## bx896x4\n",
        "\n",
        "\n",
        "  #### Finally return both score and bounding boxes\n",
        "\n",
        "\n",
        "  return tf.keras.models.Model(inputs, [x_conf, x_bboxes])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnSIwr9JWNs4",
        "colab_type": "code",
        "outputId": "9cd66158-907a-4ca5-89dc-9a661c306a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = network()\n",
        "print(model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 64, 64, 24)   1824        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 64, 64, 24)   1176        conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 64, 64, 24)   96          separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 64, 24)   0           batch_normalization_3[0][0]      \n",
            "                                                                 conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 24)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 64, 64, 24)   1176        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 24)   96          separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 64, 64, 24)   0           batch_normalization_4[0][0]      \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 64, 64, 24)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 24)   0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 32, 32, 48)   1752        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ZerosLike (TensorFl [(None, 32, 32, 24)] 0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(None, 32, 32, 48)] 0           max_pooling2d_1[0][0]            \n",
            "                                                                 tf_op_layer_ZerosLike[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
            "                                                                 tf_op_layer_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 48)   0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 32, 32, 48)   3504        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 48)   0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 32, 32, 48)   3504        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 32, 32, 48)   0           batch_normalization_7[0][0]      \n",
            "                                                                 activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 48)   0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 16, 16, 24)   2352        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 24)   96          separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 24)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 48)   0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_9 (SeparableCo (None, 16, 16, 96)   2904        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ZerosLike_1 (Tensor [(None, 16, 16, 48)] 0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 96)   384         separable_conv2d_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 16, 16, 96)] 0           max_pooling2d_2[0][0]            \n",
            "                                                                 tf_op_layer_ZerosLike_1[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 16, 16, 96)   0           batch_normalization_9[0][0]      \n",
            "                                                                 tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 96)   0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_10 (SeparableC (None, 16, 16, 24)   4704        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 24)   96          separable_conv2d_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 24)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_11 (SeparableC (None, 16, 16, 96)   2904        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 96)   384         separable_conv2d_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 96)   0           batch_normalization_11[0][0]     \n",
            "                                                                 activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 96)   0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_12 (SeparableC (None, 16, 16, 24)   4704        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 24)   96          separable_conv2d_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 24)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_13 (SeparableC (None, 16, 16, 96)   2904        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 96)   384         separable_conv2d_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 16, 16, 96)   0           batch_normalization_13[0][0]     \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 96)   0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_14 (SeparableC (None, 8, 8, 24)     4704        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 24)     96          separable_conv2d_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 24)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_15 (SeparableC (None, 8, 8, 96)     2904        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 96)     384         separable_conv2d_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 96)     0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 8, 8, 96)     0           batch_normalization_15[0][0]     \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 96)     0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_16 (SeparableC (None, 8, 8, 24)     4704        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 24)     96          separable_conv2d_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 24)     0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_17 (SeparableC (None, 8, 8, 96)     2904        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 96)     384         separable_conv2d_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 8, 8, 96)     0           batch_normalization_17[0][0]     \n",
            "                                                                 activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 96)     0           add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_18 (SeparableC (None, 8, 8, 24)     4704        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 24)     96          separable_conv2d_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 24)     0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_19 (SeparableC (None, 8, 8, 96)     2904        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 96)     384         separable_conv2d_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 8, 8, 96)     0           batch_normalization_19[0][0]     \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 96)     0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 16, 16, 2)    1730        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 8, 8, 6)      5190        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 8)    6920        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 8, 8, 24)     20760       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 512, 1)       0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 384, 1)       0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 512, 4)       0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 384, 4)       0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 896, 1)       0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 896, 4)       0           reshape_2[0][0]                  \n",
            "                                                                 reshape_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 94,480\n",
            "Trainable params: 92,656\n",
            "Non-trainable params: 1,824\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBqe_GQD46Fw",
        "colab_type": "text"
      },
      "source": [
        "#Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rV_p0ww49ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "inp = model.input                                           # input placeholder\n",
        "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
        "# print(outputs)\n",
        "functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaD_s3t15Elb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4c6a7d9-cf5f-4cdf-f794-351d17d9abf5"
      },
      "source": [
        "#Testing\n",
        "test = np.random.random((8,128,128,3))\n",
        "for func in functors:\n",
        "  print(func([test])[0].shape)\n",
        "# layer_outs = [func([test]) for func in functors]\n",
        "# print (layer_outs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 128, 128, 3)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 64, 64, 24)\n",
            "(8, 32, 32, 24)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 24)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 32, 32, 48)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 48)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 48)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 24)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 16, 16, 96)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 24)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 8, 8, 96)\n",
            "(8, 16, 16, 2)\n",
            "(8, 8, 8, 6)\n",
            "(8, 16, 16, 8)\n",
            "(8, 8, 8, 24)\n",
            "(8, 512, 1)\n",
            "(8, 384, 1)\n",
            "(8, 512, 4)\n",
            "(8, 384, 4)\n",
            "(8, 896, 1)\n",
            "(8, 896, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-wFQRWOGYG2",
        "colab_type": "text"
      },
      "source": [
        "#Calculate IoU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJK56UDoN_Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_intersection(bboxA, bboxB):\n",
        "  '''\n",
        "  bboxA:[A,4]\n",
        "  bboxB:[B,4]\n",
        "  \n",
        "  returns intersection of the boxes A and B of shape [A,B]\n",
        "  '''\n",
        "  numboxes_A = bboxA.shape[0]\n",
        "  numboxes_B = bboxB.shape[0]\n",
        "  max_xy = tf.math.minimum(tf.tile(tf.expand_dims(bboxA[:,2:],axis=1), multiples = (1,numboxes_B,1)),tf.tile(tf.expand_dims(bboxB[:,2:],axis=0), multiples = (numboxes_A,1,1)))\n",
        "  min_xy = tf.math.maximum(tf.tile(tf.expand_dims(bboxA[:,:2],axis=1), multiples = (1,numboxes_B,1)),tf.tile(tf.expand_dims(bboxB[:,:2],axis=0), multiples = (numboxes_A,1,1)))\n",
        "  intersections = tf.clip_by_value((max_xy - min_xy),clip_value_min = 0, clip_value_max = 16384)\n",
        "  return intersections[:,:,0]*intersections[:,:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuA28VO3OJBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_IoU(bboxA, bboxB):\n",
        "  '''\n",
        "  bboxA:[A,4]\n",
        "  bboxB:[B,4]\n",
        "  bbox_format: (xmin,ymin,xmax,ymax) \n",
        "\n",
        "  returns IoU of shape [A,B]\n",
        "  Note: If bbox is given in (x,y,w,h) format needs to be converted to (xmin,ymin,xmax,ymax)\n",
        "  '''\n",
        "  numboxes_A = bboxA.shape[0]\n",
        "  numboxes_B = bboxB.shape[0]\n",
        "  area_a = tf.tile(tf.expand_dims((bboxA[:,2]-bboxA[:,0])*(bboxA[:,3]-bboxA[:,1]), axis=1), multiples = (1,numboxes_B))\n",
        "  area_b = tf.tile(tf.expand_dims((bboxB[:,2]-bboxB[:,0])*(bboxB[:,3]-bboxB[:,1]), axis=0), multiples = (numboxes_A,1))\n",
        "  intersection = get_intersection(bboxA, bboxB)\n",
        "  union = area_a + area_b - intersection\n",
        "  return intersection/union"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OxQsBZ8o8vy",
        "colab_type": "text"
      },
      "source": [
        "#Prediction Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVeaoK9hBf54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preds_to_coordinates(raw_box_preds, priors):\n",
        "  '''\n",
        "  raw_box_preds: (b, 896, 4)\n",
        "  priors: (896,4)\n",
        "  decodes raw predictions from network to coordinates (xmin, ymin, xmax, ymax)\n",
        "  '''\n",
        "  # print(type(raw_box_preds))\n",
        "  bbox = np.zeros_like(raw_box_preds.numpy())\n",
        "  xy_center = (raw_box_preds[:,:,:2] / 128) * priors[:,2:] + priors[:,:2] \n",
        "  wh = (raw_box_preds[:,:,2:] / 128) * priors[:,2:]\n",
        "  bbox[:,:,:2] = (xy_center - wh/2).numpy() ####(xmin, ymin)\n",
        "  bbox[:,:,2:] = (xy_center + wh/2).numpy() ####(xmax, ymax)\n",
        "  return tf.convert_to_tensor(bbox)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGwE1yH0eoeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_detections_from_preds(raw_score_preds, raw_box_preds, priors, conf_thresh = 0.6):\n",
        "  '''\n",
        "  raw_box_preds: (b, 896, 4)\n",
        "  priors: (896,4)\n",
        "  raw_score_preds: (b, 896, 1)\n",
        "  returns detections of shape (b, num_detections, 5)\n",
        "  '''\n",
        "\n",
        "  raw_detection_boxes = preds_to_coordinates(raw_box_preds, priors)\n",
        "  # raw_detection_score = tf.sigmoid(tf.squeeze(tf.clip_by_value(raw_score_preds, clip_value_min = -clip_val, clip_value_max = clip_val),axis=-1))\n",
        "  raw_detection_score = tf.squeeze(raw_score_preds, axis=-1)\n",
        "  mask = raw_detection_score >= conf_thresh #### Discard boxes with too low confidence\n",
        "  # print(mask.shape, type(mask))\n",
        "\n",
        "  filtered_output = []\n",
        "\n",
        "  for i in range(raw_box_preds.shape[0]):\n",
        "    boxes = raw_detection_boxes.numpy()[i, mask[i]]\n",
        "    scores = raw_detection_score.numpy()[i, mask[i]]\n",
        "    filtered_output.append(tf.concat([tf.convert_to_tensor(boxes), tf.expand_dims(tf.convert_to_tensor(scores),axis=-1)], axis=-1))\n",
        "\n",
        "  return filtered_output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUG-IUEk56Fo",
        "colab_type": "text"
      },
      "source": [
        "#Test get_detections_from_preds module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuWShp8W6Ath",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fb9801d8-7799-4ff2-8e0a-c28fa32f7cc2"
      },
      "source": [
        "priors = tf.convert_to_tensor(np.random.randn(896,4),dtype = tf.float32)\n",
        "raw_score_preds = tf.convert_to_tensor(np.random.randn(8,896,1),dtype = tf.float32)\n",
        "# print(raw_score_preds)\n",
        "raw_box_preds = tf.convert_to_tensor(np.random.randn(8,896,4),dtype = tf.float32)\n",
        "fil_out = get_detections_from_preds(raw_score_preds, raw_box_preds, priors)\n",
        "print(len(fil_out), fil_out[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 tf.Tensor(\n",
            "[[-0.8367777  -0.5482367  -0.832338   -0.5485718   1.3645463 ]\n",
            " [ 0.761816    0.35967472  0.75716007  0.3601608   0.8008757 ]\n",
            " [ 0.6262133  -0.01908054  0.6271714  -0.01190832  1.2597277 ]\n",
            " ...\n",
            " [-0.8179705  -2.0137105  -0.81657267 -2.0062423   1.9174443 ]\n",
            " [-1.1382604   0.9375633  -1.1271114   0.93559575  1.3730727 ]\n",
            " [ 0.12569831  0.70423895  0.12703599  0.7020808   0.65729284]], shape=(235, 5), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-urEyRfaO-Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_nms(filtered_boxes, IoU_thresh = 0.5):\n",
        "  '''\n",
        "  filtered_boxes:(num_detections,5)\n",
        "  For each image with overlapping boxes with IoU above\n",
        "  a certain threshold it weighs the coordinates by the\n",
        "  normalized confidence score.\n",
        "  '''\n",
        "  detection_out = []\n",
        "\n",
        "  if (len(filtered_boxes) == 0):\n",
        "     return detection_out\n",
        "\n",
        "  sorted_ind = tf.argsort(filtered_boxes[:,-1], direction='DESCENDING')\n",
        "\n",
        "  while(len(sorted_ind)):\n",
        "    max_score_det = filtered_boxes[sorted_ind[0]]\n",
        "    max_score_box = tf.expand_dims(max_score_det[:4], axis = 0)\n",
        "    remaining_boxes = tf.gather_nd(filtered_boxes[:,:4], tf.expand_dims(sorted_ind,axis=-1))\n",
        "    \n",
        "    IoUs = get_IoU(max_score_box, remaining_boxes)\n",
        "    mask = tf.squeeze(IoUs > IoU_thresh, axis=0)\n",
        "    curr_overlaps = sorted_ind[mask]\n",
        "    sorted_ind = sorted_ind[~mask]\n",
        "    \n",
        "    weighted_detection = max_score_det.numpy()\n",
        "    if(curr_overlaps.shape[0] > 1):\n",
        "      score = tf.gather_nd(filtered_boxes[:,4], tf.expand_dims(curr_overlaps,axis=-1))\n",
        "      coordinates = tf.gather_nd(filtered_boxes[:,:4], tf.expand_dims(curr_overlaps,axis=-1))\n",
        "      normalized_score = score / tf.math.reduce_sum(score)\n",
        "      weighted_bbox = tf.math.reduce_sum(coordinates * normalized_score, axis = 0)\n",
        "      weighted_detection[:4] = weighted_bbox.numpy()\n",
        "      weighted_detection[4] =  (tf.math.reduce_sum(score) / curr_overlaps.shape[0]).numpy()\n",
        "    \n",
        "    detection_out.append(tf.convert_to_tensor(weighted_detection))\n",
        "\n",
        "  \n",
        "  return detection_out\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uRN7MvH6wzI",
        "colab_type": "text"
      },
      "source": [
        "#Test Weighted NMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Er1ltrI61Wg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8e22a196-2acc-47a1-f8ad-44e88e78dde0"
      },
      "source": [
        "fil_boxes = tf.convert_to_tensor([[0.11111,0.22222, 0.33333, 0.44444, 0.65], [0.222,0.333, 0.444, 0.55, 0.7], [0.11,0.33, 0.44, 0.6, 0.5], [0.1,0.23, 0.44, 0.75, 0.68], [0.222,0.333, 0.444, 0.55, 0.8],[0.2,0.4,0.6,0.8,0.2]])\n",
        "det = weighted_nms(fil_boxes, IoU_thresh = 0.25)\n",
        "print(type(det), det)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> [<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
            "array([0.19522387, 0.32022387, 0.448597  , 0.4570895 , 0.67      ],\n",
            "      dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0.11111, 0.22222, 0.33333, 0.44444, 0.65   ], dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=array([0.2, 0.4, 0.6, 0.8, 0.2], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O1YHcPSW0_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inputs, priors):\n",
        "  '''\n",
        "  inputs: tensor of shape (b,H,W,C)\n",
        "\n",
        "  returns prediction for the input of shape (b,num_detections,5)\n",
        "  Note: If no face is detected in the image then a tensor of shape (0,5) is added for that image\n",
        "  '''\n",
        "\n",
        "  model = network()\n",
        "  out = model.predict(inputs)\n",
        "  # print(type(out[0]), type(out[1]))\n",
        "  raw_detections = get_detections_from_preds(tf.convert_to_tensor(out[0]), tf.convert_to_tensor(out[1]), priors)\n",
        "  \n",
        "  final_detections = []\n",
        "\n",
        "  for i in range(inputs.shape[0]):\n",
        "    faces = weighted_nms(raw_detections[i])\n",
        "    if(len(faces)>0):\n",
        "      faces = tf.stack(faces)\n",
        "    else:\n",
        "      faces = tf.zeros([0,5])\n",
        "    final_detections.append(faces)\n",
        "  return final_detections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAF-LgJd7II1",
        "colab_type": "text"
      },
      "source": [
        "#Test Prediction Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFdhKBD47PH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdb2376e-46ac-4a4b-d52c-d6e73305ebfd"
      },
      "source": [
        "x = tf.convert_to_tensor(np.zeros((8,128,128,3)),dtype = tf.float32)\n",
        "priors = tf.convert_to_tensor(np.random.randn(896,4),dtype = tf.float32)\n",
        "detections = predict(x, priors)\n",
        "print(len(detections), detections[0].shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 (0, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s13tYJNnpDRA",
        "colab_type": "text"
      },
      "source": [
        "#Training Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ZgjXHpnnXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_loss(ground_truth, predictions):\n",
        "  '''\n",
        "  ground_truth:[(b,num_priors,4),,(b,num_priors)]\n",
        "  predictions:[(b,num_priors,4),(b,num_priors,1)]\n",
        "\n",
        "  Note that we can save priors beforehand\n",
        "  returns a scalar smooth_l1_loss\n",
        "\n",
        "  Note: priors can be saved once and for all of shape (896,4) of format (x,y,w,h)\n",
        "  '''\n",
        "\n",
        "\n",
        "  matched_loc_offset, matched_conf = ground_truth\n",
        "  pred_offset, pred_conf = predictions\n",
        "  batch_size = matched_conf.shape[0]\n",
        "  num_priors = matched_conf.shape[1]\n",
        "\n",
        "\n",
        "  pos_class = matched_conf > 0.0\n",
        "  num_pos = tf.math.reduce_sum(tf.cast(pos_class, dtype = tf.float32), axis=1) ###(b,)\n",
        "\n",
        "  pred_offset = pred_offset[pos_class]\n",
        "  matched_loc_offset = matched_loc_offset[pos_class]\n",
        "\n",
        "\n",
        "  ##### Smooth L1 loss(localisation loss)\n",
        "  h = tf.keras.losses.Huber() ##### returns averaged loss\n",
        "  N = tf.math.reduce_sum(num_pos)\n",
        "  loc_loss = (h(matched_loc_offset, pred_offset) * (batch_size * num_priors * 4))\n",
        "\n",
        "  ####TODO: Hard Negative Mining \n",
        "\n",
        "  ##### Confidence Loss\n",
        "  bce = tf.keras.losses.BinaryCrossentropy()\n",
        "  conf_loss = bce(matched_conf, pred_conf) * (batch_size * num_priors)\n",
        "\n",
        "  averaged_loss = (loc_loss + conf_loss)/N\n",
        "\n",
        "  return averaged_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDIZYYak8F5i",
        "colab_type": "text"
      },
      "source": [
        "# Test Weighted Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufbuNuh8MPw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "378a6f20-60bc-4819-c6a9-47bb3da2418d"
      },
      "source": [
        "# ground_truth = [tf.convert_to_tensor(np.random.randn(6,896,4),dtype = tf.float32), [tf.convert_to_tensor(np.random.randn(6,896),dtype = tf.float32)]]\n",
        "ground_truth = [tf.convert_to_tensor(np.random.randn(6,896,4),dtype = tf.float32), tf.ones([6,896],dtype = tf.float32)]\n",
        "predictions = [tf.convert_to_tensor(np.random.randn(6,896,4),dtype = tf.float32), tf.convert_to_tensor(np.random.randn(6,896,1),dtype = tf.float32)]\n",
        "\n",
        "loss = weighted_loss(ground_truth, predictions)\n",
        "print(loss)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(11.061824, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4vcDRZ1vg10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_offsets(matches, priors):\n",
        "  '''\n",
        "  matches:(num_priors,4), coordinate format:(xmin,ymin,xmax,ymax)\n",
        "  priors:(num_priors,4)\n",
        "\n",
        "  returns offsets of matches to priors of shape (num_priors,4) for regression\n",
        "  '''\n",
        "\n",
        "  ghat_cxcy = (matches[:,:2] + matches[:,2:])/(2 * 128) - priors[:,:2]\n",
        "  ghat_cxcy /= priors[:,2:]\n",
        "\n",
        "  ghat_wh = tf.math.log((matches[:,2:]-matches[:,:2])/(128*priors[:,2:]))\n",
        "\n",
        "  return tf.concat([ghat_cxcy, ghat_wh], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODGrU9UMR8Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match(gt_box, gt_labels, priors, overlap_thresh = 0.5):\n",
        "  '''\n",
        "  gt_box:(num_objects, 4)\n",
        "  gt_labels:(num_objects)\n",
        "  matched_loc_offset:tensor of shape (num_priors,4) to be filled with offset for each prior\n",
        "  matched_conf:tensor of shape (num_priors) to be filled with labels for each prior\n",
        "\n",
        "  It matches each default box to a ground_truth box. \n",
        "  First assign each ground truth box a prior box with max IoU\n",
        "  and then assign each prior box a ground truth box with IoU greater than 0.5.\n",
        "  \n",
        "  '''\n",
        "\n",
        "  overlaps = get_IoU(gt_box, priors)\n",
        "  \n",
        "  #### assigns each gt_box a prior_box with max IoU\n",
        "  best_prior_overlap = tf.math.reduce_max(overlaps, axis = 1)\n",
        "  best_prior_idx = tf.math.argmax(overlaps, axis = 1)\n",
        "\n",
        "  #### assigns each prior_box a gt_box with max IoU\n",
        "  best_truth_overlap = tf.math.reduce_max(overlaps, axis = 0)\n",
        "  best_truth_idx = tf.math.argmax(overlaps, axis = 0).numpy() #### assignment to eagertensor is  not possible so convert to numpy \n",
        "\n",
        "\n",
        "  #### ensure best match isn't missed due to thresholding\n",
        "  fill_value = tf.cast(tf.fill(best_prior_idx.shape[0], 3),dtype = best_truth_overlap.dtype)\n",
        "  best_truth_overlap = tf.tensor_scatter_nd_update(best_truth_overlap, tf.expand_dims(best_prior_idx, axis=1), fill_value)\n",
        "\n",
        "  for i in range(best_prior_idx.shape[0]):\n",
        "    best_truth_idx[best_prior_idx[i]] = i\n",
        "  \n",
        "  best_truth_idx = tf.convert_to_tensor(best_truth_idx)\n",
        "\n",
        "\n",
        "  matches = tf.gather_nd(gt_box, tf.expand_dims(best_truth_idx, axis=1))\n",
        "  labels = (tf.gather_nd(gt_labels, tf.expand_dims(best_truth_idx, axis=1)) + 1).numpy() #### assignment to eagertensor is  not possible so convert to numpy\n",
        "\n",
        "\n",
        "  bg_mask =  best_truth_overlap < overlap_thresh #### priors with IoU < threshold are background\n",
        "  labels[bg_mask] = 0 ### background class is 0\n",
        "\n",
        "  return [get_offsets(matches, priors), tf.convert_to_tensor(labels)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2IkIun89ZDq",
        "colab_type": "text"
      },
      "source": [
        "#Test Matching Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcMaEoaZoGPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "7be645e9-8829-474f-e73b-50e21b3ba9cc"
      },
      "source": [
        "ground_truth_box = tf.convert_to_tensor(np.random.randn(5,4),dtype = tf.float32)\n",
        "ground_truth_labels = tf.zeros([5],dtype = tf.float32)\n",
        "priors = tf.convert_to_tensor(np.random.randn(896,4),dtype = tf.float32)\n",
        "\n",
        "\n",
        "matched_loc_offset, matched_conf = match(ground_truth_box, ground_truth_labels, priors)\n",
        "\n",
        "print(matched_conf,matched_loc_offset.shape, matched_conf.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.], shape=(896,), dtype=float32) (896, 4) (896,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK0X8Lha-KEE",
        "colab_type": "text"
      },
      "source": [
        "#Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abw4QjciBzdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_dir = '/home/subh/blazeface_/images'\n",
        "priors_path = '/home/subh/blazeface_/priors.npy'\n",
        "labels_path = '/home/subh/blazeface_/labels.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O5ZDuOp9gar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_img(img_path, img_size=(128,128)):\n",
        "    \"\"\"\n",
        "    load image file and rescale it by 255.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, img_shape)\n",
        "    img /= 255.\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOhv0bPd-Pkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(overlap_thresh = 0.5, img_size = (128,128)):\n",
        "  '''\n",
        "  returns image, bbox_offset_with_labels\n",
        "  '''\n",
        "\n",
        "  img_files = os.listdir(img_dir)\n",
        "\n",
        "  with open(labels_path, 'rb') as f:\n",
        "    labels = pickle.load(f)\n",
        "\n",
        "  numofData = len(img_files)\n",
        "  data_indices = np.arange(numofData)\n",
        "\n",
        "  priors = tf.convert_to_tensor(np.load(priors_path), dtype=tf.float32)\n",
        "  \n",
        "  i=0\n",
        "  while i<numofData :\n",
        "    img = get_img(os.path.join(img_dir, img_files[i]))\n",
        "    labels = labels[i]\n",
        "    matched_loc_offset,matched_conf = match(labels[:4], labels[4], priors)\n",
        "\n",
        "    yield matched_loc_offset, matched_conf\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol7ChPdv-Txf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_data = tf.data.Dataset.from_generator(data_generator,  output_types = (tf.float32,tf.float32), output_shapes = ((896,4),(896,))).shuffle(100).batch(8)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}